# 1.æ•°æ®é›†æ„å»º

## 1.ä¸åŒæ•°æ®é›†è½¬åŒ–

### 1.cocoæ•°æ®é›†æ„å»º

#### 1.å…ˆä½¿ç”¨labelmeè¿›è¡Œæ ‡æ³¨

#### 2.ä½¿ç”¨ä»¥ä¸‹ä»£ç å°†æ•°æ®è½¬åŒ–æˆcocoæ•°æ®é›†

```python
import os
import json
import argparse
import numpy as np
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# ================== å‚æ•°è§£æ ==================
parser = argparse.ArgumentParser()
parser.add_argument('--root_dir', type=str, default="C:\\Users\\lenovo\\Desktop\\æ–°å»ºæ–‡ä»¶å¤¹\\labelme_result",
                    help="æ ¹ç›®å½•è·¯å¾„ï¼ˆåŒ…å«æ‰€æœ‰labelmeæ ‡æ³¨çš„jsonæ–‡ä»¶ï¼‰")
parser.add_argument('--save_path', type=str, default="C:\\Users\\lenovo\\Desktop\\æ–°å»ºæ–‡ä»¶å¤¹\\coco.json",
                    help="ä¿å­˜è·¯å¾„ï¼ˆä¸åˆ’åˆ†æ•°æ®é›†æ—¶ä½¿ç”¨ï¼‰")
parser.add_argument('--random_split', action='store_true', default=True,
                    help="å¯ç”¨éšæœºåˆ’åˆ†æ•°æ®é›†ï¼ˆé»˜è®¤æ¯”ä¾‹8:1:1ï¼‰")
args = parser.parse_args()


# ================== æ•°æ®é›†åˆ’åˆ†å‡½æ•° ==================
def split_dataset(data, test_size=0.1, val_size=0.1):
    """
    åˆ’åˆ†æ•°æ®é›†ä¸ºtrain/val/test (æ¯”ä¾‹ 8:1:1)
    """
    # ç¬¬ä¸€æ¬¡åˆ’åˆ†ï¼šåˆ†å‡ºè®­ç»ƒé›†
    train_data, temp_data = train_test_split(
        data,
        test_size=test_size + val_size,
        random_state=42
    )

    # ç¬¬äºŒæ¬¡åˆ’åˆ†ï¼šä»å‰©ä½™æ•°æ®ä¸­åˆ†å‡ºéªŒè¯é›†å’Œæµ‹è¯•é›†
    val_data, test_data = train_test_split(
        temp_data,
        test_size=test_size / (test_size + val_size),
        random_state=42
    )

    return train_data, val_data, test_data


# ================== ä¸»è½¬æ¢å‡½æ•° ==================
def labelme2coco():
    # åˆå§‹åŒ–æ•°æ®ç»“æ„
    coco_template = {
        "info": {"description": "COCO Dataset"},
        "licenses": [],
        "categories": [],
        "images": [],
        "annotations": []
    }

    # æ”¶é›†æ‰€æœ‰æ ‡æ³¨æ•°æ®å’Œç±»åˆ«
    all_data = []
    category_map = {}
    image_id_map = {}

    print("æ­£åœ¨æ‰«ææ ‡æ³¨æ–‡ä»¶...")
    # éå†æ ¹ç›®å½•ä¸‹çš„æ‰€æœ‰jsonæ–‡ä»¶
    # éå†æ ¹ç›®å½•ä¸‹çš„æ‰€æœ‰jsonæ–‡ä»¶
    for idx, filename in enumerate(tqdm(os.listdir(args.root_dir))):
        if filename.endswith(".json"):
            filepath = os.path.join(args.root_dir, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                    # è®°å½•å›¾åƒè·¯å¾„æ˜ å°„
                    image_id_map[data["imagePath"]] = idx

                    # æ”¶é›†ç±»åˆ«ä¿¡æ¯
                    for shape in data["shapes"]:
                        label = shape["label"]
                        if label not in category_map:
                            # æ–°å¢é¢œè‰²ç›¸å…³å­—æ®µ
                            category_map[label] = {
                                "id": len(category_map),
                                "keypoints": [f"point_{i + 1}" for i in range(len(shape["points"]))],
                                "skeleton": [[i + 1, (i + 1) % len(shape["points"]) + 1] for i in
                                             range(len(shape["points"]))],
                                "colors": {},  # é¢œè‰²åç§°åˆ°IDçš„æ˜ å°„
                                "color_list": []  # é¢œè‰²åç§°é¡ºåºåˆ—è¡¨
                            }
                        # å¤„ç†é¢œè‰²ä¿¡æ¯
                        flags = shape.get("flags", {})
                        for flag_key in flags:
                            if flag_key.startswith("color_") and flags[flag_key]:
                                color_name = flag_key[6:]  # å»é™¤"color_"å‰ç¼€
                                # å¦‚æœé¢œè‰²ä¸å­˜åœ¨åˆ™æ·»åŠ 
                                if color_name not in category_map[label]["colors"]:
                                    color_id = len(category_map[label]["color_list"])
                                    category_map[label]["colors"][color_name] = color_id
                                    category_map[label]["color_list"].append(color_name)
                        # ...å…¶ä»–å¤„ç†...
                    all_data.append(data)
            except Exception as e:
                print(f"é”™è¯¯åŠ è½½æ–‡ä»¶ {filename}: {str(e)}")
                continue

    # æ„å»ºcategories
    for label, info in category_map.items():
        # è½¬æ¢é¢œè‰²ä¿¡æ¯ä¸ºåˆ—è¡¨æ ¼å¼
        color_info = [{"id": idx, "name": name} for idx, name in enumerate(info["color_list"])]
        coco_template["categories"].append({
            "id": info["id"],
            "name": label,
            "supercategory": "object",
            "keypoints": info["keypoints"],
            "skeleton": info["skeleton"],
            "colors": color_info  # æ–°å¢é¢œè‰²å­—æ®µ
        })

    # æ•°æ®é›†åˆ’åˆ†
    if args.random_split:
        train_data, val_data, test_data = split_dataset(all_data)
        datasets = {
            "train": train_data,
            "val": val_data,
            "test": test_data
        }
    else:
        datasets = {"all": all_data}

    # å¤„ç†æ¯ä¸ªæ•°æ®é›†åˆ’åˆ†
    for phase, data in datasets.items():
        coco_data = coco_template.copy()
        coco_data["images"] = []
        coco_data["annotations"] = []
        ann_id = 0

        print(f"æ­£åœ¨å¤„ç† {phase} æ•°æ®é›†...")
        for img_data in tqdm(data):
            # æ„å»ºimageä¿¡æ¯
            image_info = {
                "id": image_id_map[img_data["imagePath"]],
                "file_name": os.path.basename(img_data["imagePath"]),
                "width": img_data["imageWidth"],
                "height": img_data["imageHeight"],
                "license": 0,
                "date_captured": ""
            }
            coco_data["images"].append(image_info)

            # å¤„ç†æ ‡æ³¨
            for shape in img_data["shapes"]:
                points = np.array(shape["points"])
                # è½¬æ¢å…³é”®ç‚¹æ ¼å¼ [x1,y1,v1,x2,y2,v2,...]
                keypoints = []
                for x, y in points:
                    keypoints.extend([float(x), float(y), 2])  # v=2è¡¨ç¤ºå¯è§

                # è®¡ç®—è¾¹ç•Œæ¡†
                x_coords = points[:, 0]
                y_coords = points[:, 1]
                bbox = [
                    float(np.min(x_coords)),  # x_min
                    float(np.min(y_coords)),  # y_min
                    float(np.max(x_coords) - np.min(x_coords)),  # width
                    float(np.max(y_coords) - np.min(y_coords))  # height
                ]

                # è®¡ç®—å¤šè¾¹å½¢é¢ç§¯ï¼ˆä½¿ç”¨shoelaceå…¬å¼ï¼‰
                area = 0.5 * abs(np.sum(
                    x_coords * np.roll(y_coords, 1) -
                    np.roll(x_coords, 1) * y_coords)
                )
                # å¤„ç†é¢œè‰²ä¿¡æ¯ï¼ˆæ–°å¢ï¼‰
                color_id = -1  # é»˜è®¤å€¼
                flags = shape.get("flags", {})
                for flag_key in flags:
                    if flag_key.startswith("color_") and flags[flag_key]:
                        color_name = flag_key[6:]
                        if color_name in category_map[label]["colors"]:
                            color_id = category_map[label]["colors"][color_name]
                            break
                # æ„å»ºannotation
                coco_data["annotations"].append({
                    "id": ann_id,
                    "image_id": image_info["id"],
                    "category_id": category_map[shape["label"]]["id"],
                    "color_id": color_id,  # æ–°å¢é¢œè‰²IDå­—æ®µ
                    "segmentation": [points.flatten().tolist()],
                    "area": float(area),
                    "bbox": bbox,
                    "iscrowd": 0,
                    "keypoints": keypoints,
                    "num_keypoints": len(points)
                })
                ann_id += 1

                # ä¿å­˜ç»“æœï¼ˆä¿®æ­£ä¿å­˜é€»è¾‘ï¼‰
                output_path = args.save_path
                if args.random_split:
                    base, ext = os.path.splitext(args.save_path)
                    output_path = f"{base}_{phase}{ext}"

                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                with open(output_path, 'w') as f:
                    json.dump(coco_data, f, indent=2)
                print(f"å·²ä¿å­˜ {phase} æ•°æ®é›†åˆ° {output_path}")

if __name__ == "__main__":
    labelme2coco()
    
```



### 2.coco-pose.yamlç”¨yolov11è®­ç»ƒçš„æ•°æ®é›†

#### 1.å…ˆä½¿ç”¨labelmeè¿›è¡Œæ ‡æ³¨

#### 2.ç”¨ä»£ç å°†labelmeæ ‡æ³¨çš„æ•°æ®é›†è¿›è¡Œè½¬åŒ–

```python
import os
import json
import argparse
import numpy as np
import shutil
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# ================== å‚æ•°è§£æ ==================
parser = argparse.ArgumentParser()
parser.add_argument('--root_dir', type=str, default="C:\\Users\\lenovo\\Desktop\\tup\\labelme_result",
                    help="æ ¹ç›®å½•è·¯å¾„ï¼ˆåŒ…å«æ‰€æœ‰ labelme æ ‡æ³¨çš„ json æ–‡ä»¶ï¼‰")
parser.add_argument('--image_dir', type=str, default="C:\\Users\\lenovo\\Desktop\\tup\\images",
                    help="å›¾ç‰‡æ‰€åœ¨ç›®å½•ï¼ˆä¸ json æ–‡ä»¶çš„ imagePath å¯¹åº”ï¼‰")
parser.add_argument('--random_split', action='store_true', default=True,
                    help="å¯ç”¨éšæœºåˆ’åˆ†æ•°æ®é›†ï¼ˆé»˜è®¤æ¯”ä¾‹ 8:1:1ï¼‰")
parser.add_argument('--output_dir', type=str, default="C:\\Users\\lenovo\\Desktop\\tup\\dataset_output",
                    help="è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆç”¨äºä¿å­˜åˆ†ç±»åçš„æ•°æ®é›†ï¼‰")
args = parser.parse_args()


# ================== æ•°æ®é›†åˆ’åˆ†å‡½æ•° ==================
def split_dataset(data, test_size=0.1, val_size=0.1):
    train_data, temp_data = train_test_split(
        data,
        test_size=test_size + val_size,
        random_state=42
    )
    val_data, test_data = train_test_split(
        temp_data,
        test_size=test_size / (test_size + val_size),
        random_state=42
    )
    return train_data, val_data, test_data


# ================== åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„ ==================
def create_output_dirs(output_dir, phases):
    for phase in phases:
        phase_dir = os.path.join(output_dir, phase)
        os.makedirs(os.path.join(phase_dir, "images"), exist_ok=True)
        os.makedirs(os.path.join(phase_dir, "labels"), exist_ok=True)


# ================== ä¸»è½¬æ¢å‡½æ•° ==================
def process_json_files():
    # è·å–æ‰€æœ‰ json æ–‡ä»¶å
    json_files = [f for f in os.listdir(args.root_dir) if f.endswith(".json")]

    # åˆ’åˆ†æ•°æ®é›†ï¼ˆæ ¹æ®æ–‡ä»¶ååˆ—è¡¨åˆ’åˆ†ï¼‰
    if args.random_split:
        train_files, val_files, test_files = split_dataset(json_files)
        datasets = {"train": train_files, "val": val_files, "test": test_files}
    else:
        datasets = {"all": json_files}

    # å…¨å±€ç±»åˆ«æ˜ å°„ï¼Œä¿è¯ç±»åˆ«å·ä¸€è‡´
    category_map = {}
    next_cat_id = 0

    for phase, file_list in datasets.items():
        phase_image_dir = os.path.join(args.output_dir, phase, "images")
        phase_label_dir = os.path.join(args.output_dir, phase, "labels")

        print(f"æ­£åœ¨å¤„ç† {phase} æ•°æ®é›†ï¼Œå…± {len(file_list)} ä¸ªæ–‡ä»¶...")
        for json_file in tqdm(file_list, desc=f"Processing {phase}"):
            json_path = os.path.join(args.root_dir, json_file)
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶ {json_file} å¤±è´¥: {e}")
                continue

            img_filename = os.path.basename(data.get("imagePath", ""))
            src_img_path = os.path.join(args.image_dir, img_filename)

            # è·å–å›¾ç‰‡å°ºå¯¸
            image_width = data.get("imageWidth", None)
            image_height = data.get("imageHeight", None)
            if image_width is None or image_height is None:
                print(f"{json_file} ä¸­ç¼ºå°‘ imageWidth æˆ– imageHeightï¼Œè·³è¿‡è¯¥æ–‡ä»¶ã€‚")
                continue

            label_lines = []
            for shape in data.get("shapes", []):
                label = shape.get("label")
                if label not in category_map:
                    category_map[label] = next_cat_id
                    next_cat_id += 1
                cat_id = category_map[label]

                points = np.array(shape.get("points", []))
                if points.size == 0:
                    continue

                # è®¡ç®— bboxï¼Œå½’ä¸€åŒ–
                x_min = np.min(points[:, 0]) / image_width
                y_min = np.min(points[:, 1]) / image_height
                x_max = np.max(points[:, 0]) / image_width
                y_max = np.max(points[:, 1]) / image_height
                x_center = (x_min + x_max) / 2.0
                y_center = (y_min + y_max) / 2.0
                width = x_max - x_min
                height = y_max - y_min

                # å½’ä¸€åŒ–å…³é”®ç‚¹
                kp_list = []
                for pt in points:
                    kp_list.append(f"{pt[0] / image_width:.6f}")
                    kp_list.append(f"{pt[1] / image_height:.6f}")

                line_elements = [str(cat_id),
                                 f"{x_center:.6f}",
                                 f"{y_center:.6f}",
                                 f"{width:.6f}",
                                 f"{height:.6f}"]
                line_elements.extend(kp_list)
                label_line = " ".join(line_elements)
                label_lines.append(label_line)

            label_filename = os.path.splitext(json_file)[0] + ".txt"
            label_file_path = os.path.join(phase_label_dir, label_filename)
            with open(label_file_path, 'w') as lf:
                lf.write("\n".join(label_lines))

            if os.path.exists(src_img_path):
                dest_img_path = os.path.join(phase_image_dir, img_filename)
                try:
                    shutil.copy2(src_img_path, dest_img_path)
                except Exception as e:
                    print(f"å¤åˆ¶å›¾ç‰‡ {img_filename} å¤±è´¥: {e}")
            else:
                print(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {src_img_path}")


if __name__ == "__main__":
    if args.random_split:
        phases = ["train", "val", "test"]
    else:
        phases = ["all"]
    create_output_dirs(args.output_dir, phases)
    process_json_files()

```







## 2.åˆ›å»ºç›®å½•

### 1.coco-pose.yamlè¦æ±‚çš„ç›®å½•æ ¼å¼

**ultralytics-main**åœ¨è¯¥ç›®å½•ä¸‹åˆ›å»ºdatasetsæ–‡ä»¶å¤¹ï¼ŒæŒ‰ç…§ä¸‹é¢çš„ç»“æ„è¿›è¡Œåˆ†å¸ƒ

```
# datasets/
# â””â”€â”€ coco8-pose/
#     â”œâ”€â”€ images/    #å›¾åƒ     
#     â”‚   â””â”€â”€ train
	  |	  |-- val
	  |   |-- test
#     â””â”€â”€ labels/    # txtæ ‡æ³¨æ–‡ä»¶
#         â”œâ”€â”€ train
#         â”œâ”€â”€ val
#         â””â”€â”€ test
```











# 2.åŸºæœ¬å‚æ•°é…ç½®

## 1.å¯ä»¥é€‰çš„

### 1.**YOLO è®­ç»ƒ/æ¨ç†çš„å…¨å±€é…ç½®**

**ultralytics-main\ultralytics\cfg\default.yaml**



### 2.æ¨¡å‹é…ç½®

**ultralytics-main\ultralytics\cfg\models**



ä¾‹å¦‚ultralytics-main\ultralytics\cfg\models\11\yolo11-pose.yaml

è¿™ä¸ªæ¨¡å‹é‡Œé¢çš„kpt_shape: å¯ä»¥è¿›è¡Œä¿®æ”¹

```yaml
nc: 80 # number of classesï¼Œæ£€æµ‹ç±»åˆ«çš„æ•°é‡
kpt_shape: [5, 2]  # 5ä¸ªå…³é”®ç‚¹ï¼Œæ¯ä¸ªç‚¹åªæœ‰(x,y)åæ ‡
```

## 2.å¿…é¡»è¦åšçš„

**ultralytics-main\ultralytics\cfg\datasets**ä»è¿™ä¸ªæ–‡ä»¶å¤¹ä¸‹é¢é€‰ä¸€ä¸ªæˆ‘ä»¬éœ€è¦çš„æ•°æ®é›†æ ¼å¼çš„.yamlæ–‡ä»¶ï¼ŒæŠŠå®ƒæ”¾åˆ°è®­ç»ƒä»£ç çš„åŒçº§ç›®å½•(æ–¹ä¾¿è§‚å¯Ÿ)

æ ¹æ®è‡ªå·±çš„éœ€æ±‚å…·ä½“ä¿®æ”¹

### 1.coco-pose.yaml

```yaml
# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

# COCO 2017 Keypoints dataset https://cocodataset.org by Microsoft
# Documentation: https://docs.ultralytics.com/datasets/pose/coco/
# Example usage: yolo train data=coco-pose.yaml
# parent
# â”œâ”€â”€ ultralytics
# â””â”€â”€ datasets
#     â””â”€â”€ coco-pose  â† downloads here (20.1 GB)

#-----------è¿™é‡Œå°±æ˜¯å¯¹åº”çš„è·¯å¾„-------------------------------
# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
path: ./datasets/coco-pose # dataset root dir
train: annotations/coco_train.json # train images (relative to 'path') 56599 images
val: annotations/coco_val.json # val images (relative to 'path') 2346 images
test: annotations/coco_test.json # 20288 of 40670 images, submit to https://codalab.lisn.upsaclay.fr/competitions/7403

#-----------------------------------------------------------

#-----------å…³é”®ç‚¹ï¼Œå’Œæ°´å¹³åè½¬åå¯¹ç§°æ–¹å‘--------------------------
# Keypoints
kpt_shape: [5, 2] #è¿™ä¸ªå°±ä»£è¡¨ç€æœ‰5ä¸ªå…³é”®ç‚¹ï¼Œå¹¶éƒ½æ˜¯(x,y)çš„å½¢å¼
flip_idx: [0,2,1,4,3] #è¿™ä¸ªä»£è¡¨ç€ï¼Œå½“å›¾åƒæ°´å¹³ç¿»è½¬åï¼Œ0ä½ç½®çš„ç‚¹è¿˜æ˜¯0ä½ç½®çš„ç‚¹ï¼Œ1ä½ç½®çš„ç‚¹å˜æˆäº†2ä½ç½®çš„ç‚¹ä»¥æ­¤ç±»æ¨
#--------------------------------------------------------


#-------------æ ‡ç­¾-------------------------------
# Classes
names:
  0: armor
#--------------------------------------------------


# Download script/URL (optional)
download: |
  from pathlib import Path

  from ultralytics.utils.downloads import download

  # Download labels
  dir = Path(yaml["path"])  # dataset root dir
  url = "https://github.com/ultralytics/assets/releases/download/v0.0.0/"
  urls = [f"{url}coco2017labels-pose.zip"]
  download(urls, dir=dir.parent)
  # Download data
  urls = [
      "http://images.cocodataset.org/zips/train2017.zip",  # 19G, 118k images
      "http://images.cocodataset.org/zips/val2017.zip",  # 1G, 5k images
      "http://images.cocodataset.org/zips/test2017.zip",  # 7G, 41k images (optional)
  ]
  download(urls, dir=dir / "images", threads=3)

```



å¯¹äºä¸Šé¢çš„flip_idxè¿™ä¸ªå€¼å¯ä»¥åˆ©ç”¨ä¸‹é¢ä»£ç ï¼Œè§‚å¯Ÿå¯¹åº”ç‚¹åè½¬ååº”è¯¥æ˜¯å¦‚ä½•å¯¹åº”

```python
import json
import os
from pathlib import Path
import numpy as np
import cv2

def check_annotations(json_path, image_dir):
    """æ£€æŸ¥COCOæ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶"""
    print(f"å¼€å§‹æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶: {json_path}")
    
    # åŠ è½½æ ‡æ³¨æ–‡ä»¶
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # 1. åŸºæœ¬ä¿¡æ¯æ£€æŸ¥
    print("\n=== åŸºæœ¬ä¿¡æ¯ ===")
    print(f"å›¾ç‰‡æ•°é‡: {len(data['images'])}")
    print(f"æ ‡æ³¨æ•°é‡: {len(data['annotations'])}")
    print(f"ç±»åˆ«æ•°é‡: {len(data['categories'])}")
    
    # 2. æ£€æŸ¥å›¾ç‰‡åˆ†è¾¨ç‡
    print("\n=== å›¾ç‰‡åˆ†è¾¨ç‡æ£€æŸ¥ ===")
    resolutions = set()
    for img in data['images']:
        resolutions.add((img['width'], img['height']))
    print(f"æ•°æ®é›†ä¸­çš„å›¾ç‰‡åˆ†è¾¨ç‡: {resolutions}")
    
    # 3. æ£€æŸ¥æ ‡æ³¨æ ¼å¼
    print("\n=== æ ‡æ³¨æ ¼å¼æ£€æŸ¥ ===")
    for i, ann in enumerate(data['annotations'][:5]):
        print(f"\næ ‡æ³¨ {i+1}:")
        print(f"å›¾ç‰‡ID: {ann['image_id']}")
        print(f"bbox: {ann['bbox']}")
        print(f"å…³é”®ç‚¹æ•°é‡: {ann['num_keypoints']}")
        print(f"å…³é”®ç‚¹åæ ‡æ•°: {len(ann['keypoints'])}")
        
        # æ£€æŸ¥bboxæ˜¯å¦åœ¨å›¾ç‰‡èŒƒå›´å†…
        img_info = next(img for img in data['images'] if img['id'] == ann['image_id'])
        x, y, w, h = ann['bbox']
        if x < 0 or y < 0 or x + w > img_info['width'] or y + h > img_info['height']:
            print(f"âš ï¸ è­¦å‘Š: bboxè¶…å‡ºå›¾ç‰‡èŒƒå›´!")
            
    # 4. éªŒè¯å›¾ç‰‡æ–‡ä»¶
    print("\n=== å›¾ç‰‡æ–‡ä»¶æ£€æŸ¥ ===")
    missing_files = []
    for img in data['images']:
        img_path = Path(image_dir) / img['file_name']
        if not img_path.exists():
            missing_files.append(img['file_name'])
    
    if missing_files:
        print("âŒ ä»¥ä¸‹å›¾ç‰‡æ–‡ä»¶ç¼ºå¤±:")
        for f in missing_files:
            print(f" - {f}")
    else:
        print("âœ… æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶éƒ½å­˜åœ¨")
        
    # 5. æ£€æŸ¥å…³é”®ç‚¹åˆ†å¸ƒ
    print("\n=== å…³é”®ç‚¹ç»Ÿè®¡ ===")
    keypoints_count = [ann['num_keypoints'] for ann in data['annotations']]
    print(f"å¹³å‡å…³é”®ç‚¹æ•°: {np.mean(keypoints_count):.2f}")
    print(f"æœ€å°å…³é”®ç‚¹æ•°: {min(keypoints_count)}")
    print(f"æœ€å¤§å…³é”®ç‚¹æ•°: {max(keypoints_count)}")

def visualize_annotations(json_path, image_dir, output_dir, num_samples=5):
    """å¯è§†åŒ–éƒ¨åˆ†æ ‡æ³¨ç»“æœï¼ŒåŒ…æ‹¬å…³é”®ç‚¹è¿çº¿å’Œé¡ºåºç¼–å·"""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    os.makedirs(output_dir, exist_ok=True)
    
    # éšæœºé€‰æ‹©å‡ å¼ å›¾ç‰‡è¿›è¡Œå¯è§†åŒ–
    import random
    sample_imgs = random.sample(data['images'], min(num_samples, len(data['images'])))
    
    # å®šä¹‰å…³é”®ç‚¹è¿æ¥å…³ç³»ï¼ˆè£…ç”²æ¿äº”ä¸ªç‚¹çš„è¿æ¥é¡ºåºï¼‰
    connections = [
        (0, 1), (1, 2), (2, 3), (3, 0),  # å¤–æ¡†å››è¾¹å½¢
        (0, 4), (1, 4), (2, 4), (3, 4)   # ä¸­å¿ƒç‚¹ä¸å››ä¸ªè§’ç‚¹çš„è¿æ¥
    ]
    
    for img_info in sample_imgs:
        img_path = Path(image_dir) / img_info['file_name']
        if not img_path.exists():
            continue
            
        img = cv2.imread(str(img_path))
        
        # æ‰¾åˆ°è¯¥å›¾ç‰‡çš„æ‰€æœ‰æ ‡æ³¨
        annotations = [ann for ann in data['annotations'] if ann['image_id'] == img_info['id']]
        
        # ç»˜åˆ¶bboxå’Œå…³é”®ç‚¹
        for ann in annotations:
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            x, y, w, h = map(int, ann['bbox'])
            cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)
            
            # è·å–æ‰€æœ‰å…³é”®ç‚¹åæ ‡
            keypoints = []
            for i in range(0, len(ann['keypoints']), 3):
                x, y, v = ann['keypoints'][i:i+3]
                if v > 0:  # åªå¤„ç†å¯è§çš„å…³é”®ç‚¹
                    keypoints.append((int(x), int(y)))
            
            # ç»˜åˆ¶å…³é”®ç‚¹ä¹‹é—´çš„è¿çº¿
            if len(keypoints) == 5:  # ç¡®ä¿æœ‰5ä¸ªå…³é”®ç‚¹
                for start_idx, end_idx in connections:
                    if start_idx < len(keypoints) and end_idx < len(keypoints):
                        cv2.line(img, 
                                keypoints[start_idx], 
                                keypoints[end_idx], 
                                (255, 0, 0),  # è“è‰²è¿çº¿
                                2)
            
            # ç»˜åˆ¶å…³é”®ç‚¹å¹¶æ·»åŠ ç¼–å·
            for idx, point in enumerate(keypoints):
                # ç»˜åˆ¶å…³é”®ç‚¹ï¼ˆçº¢è‰²ï¼‰
                cv2.circle(img, point, 5, (0, 0, 255), -1)
                # æ·»åŠ å…³é”®ç‚¹ç¼–å·ï¼ˆç™½è‰²èƒŒæ™¯ï¼Œé»‘è‰²æ–‡å­—ï¼‰
                text_position = (point[0] + 10, point[1] - 10)
                # å…ˆç»˜åˆ¶ç™½è‰²èƒŒæ™¯çŸ©å½¢
                text_size = cv2.getTextSize(str(idx), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
                cv2.rectangle(img, 
                            (text_position[0] - 2, text_position[1] - text_size[1] - 2),
                            (text_position[0] + text_size[0] + 2, text_position[1] + 2),
                            (255, 255, 255), -1)
                # å†ç»˜åˆ¶é»‘è‰²æ–‡å­—
                cv2.putText(img, str(idx), text_position, 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
                
        # æ·»åŠ å›¾ä¾‹
        legend_y = 30
        cv2.putText(img, 'Red: Keypoints', (10, legend_y), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        cv2.putText(img, 'Blue: Connections', (10, legend_y + 30), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)
        cv2.putText(img, 'Green: Bounding Box', (10, legend_y + 60), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        cv2.putText(img, 'White: Keypoint Index', (10, legend_y + 90), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # ä¿å­˜ç»“æœ
        output_path = Path(output_dir) / f"vis_{img_info['file_name']}"
        cv2.imwrite(str(output_path), img)
        
    print(f"âœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜è‡³: {output_dir}")

if __name__ == "__main__":
    json_path = "C:\\Users\\lenovo\\Desktop\\tup\\coco\\coco_train.json"
    image_dir = "C:\\Users\\lenovo\\Desktop\\tup\\images"
    output_dir = "D:\\tup_yolox\\visualization_results"
    
    check_annotations(json_path, image_dir)
    visualize_annotations(json_path, image_dir, output_dir)
```







# 3.ä½¿ç”¨ä»£ç è®­ç»ƒ

```python
from ultralytics import YOLO

#ä¸»è¦ä»»åŠ¡ç±»å‹ï¼š
#-detect: ç›®æ ‡æ£€æµ‹ï¼ˆçŸ©å½¢æ¡†ï¼‰
#- pose: å§¿æ€ä¼°è®¡ï¼ˆå…³é”®ç‚¹ï¼‰
#- segment: å®ä¾‹åˆ†å‰²ï¼ˆåƒç´ çº§ï¼‰
#- classify: å›¾åƒåˆ†ç±»
#åŠ è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªæ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„ä½ç½®ï¼Œç¬¬äºŒä¸ªæ˜¯ä»»åŠ¡æ¨¡å¼(è¿™é‡Œæ˜¯æ£€æµ‹ä»»åŠ¡)
#model = YOLO("E:\\ultralytics-main\\yolo11n.pt",task="detect")

model = YOLO("E:\\ultralytics-main\\yolo11n.pt",task="pose")

#ç¬¬ä¸€ä¸ªå‚æ•°å°±æ˜¯è®­ç»ƒç”¨çš„æ•°æ®é›†æè¿°æ–‡ä»¶ï¼Œepochså°±æ˜¯è®­ç»ƒå¤šå°‘è½®ï¼Œworkerså¿…é¡»è¦è®¾ç½®(windowsè®¾ç½®æˆ1å°±è¡Œ)
#batchæ ¹æ®ç”µè„‘æ˜¾å¡é…ç½®è¿›è¡Œè°ƒæ•´
results = model.train(data='./coco-pose.yaml',epochs=50,workers=1,batch=16)
```

















# ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹

```python
from ultralytics import YOLO

yolo = YOLO("è®­ç»ƒå¥½çš„æ¨¡å‹",task="detect")

#æµ‹è¯•å›¾ç‰‡
result = yolo(source="æµ‹è¯•å›¾ç‰‡è·¯å¾„",show=True,save=True)
#æµ‹è¯•è§†é¢‘
result = yolo.predict(source="æµ‹è¯•è§†é¢‘è·¯å¾„",show=True,save=True)
```

è¿™é‡Œå»ºè®®åœ¨jupyteré‡Œè¿›è¡Œ

ç„¶åå¯ä»¥ç›´æ¥è¿è¡Œ

```python
result[0]#resultæ˜¯åˆ—è¡¨ï¼Œæ‰€ä»¥è¯´å¦‚æœæµ‹è¯•çš„æ˜¯å›¾ç‰‡çš„è¯ï¼Œå°±å¯ä»¥åªå–ç¬¬ä¸€ä¸ªè¾“å‡ºç»“æœ
#å¦‚æœå¤„ç†çš„è§†é¢‘çš„è¯ï¼Œè¿™é‡Œçš„æ„æ€å°±æ˜¯è¾“å‡ºçš„ç¬¬ä¸€å¸§çš„è¾“å‡ºç»“æœ
```

çœ‹åˆ°resulté‡Œé¢çš„å…·ä½“æ•°æ®éƒ½æœ‰ä»€ä¹ˆ









# å¯¼å‡ºæ¨¡å‹ä¸ºONNX

```python
from ultralytics import YOLO

model = YOLO(r"runs\detect\train6\weights\best.pt")

model.export(
    format="onnx",
    dynamic=True,      # åŠ¨æ€è¾“å…¥å°ºå¯¸ & batch
    simplify=True,     # ç®€åŒ–æ¨¡å‹
    opset=17,          # ONNX opset
    imgsz=640,         # è¾“å…¥åˆ†è¾¨ç‡
    batch=1            # é»˜è®¤ batch
)

```